{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dbc356e",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "Install the necessary libraries for our simple RAG system.\n",
    "\n",
    "| Library | What it does |\n",
    "|---------|--------------|\n",
    "| **requests** | Makes HTTP calls to LM Studio's API (like a messenger between your code and the LLM) |\n",
    "| **sentence-transformers** | Converts text into numerical vectors (embeddings) so we can measure similarity between questions and documents |\n",
    "| **chromadb** | A vector database that stores embeddings and finds the most similar documents quickly |\n",
    "| **pypdf** | Reads and extracts text from PDF files |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afe8b17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests sentence-transformers chromadb pypdf -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f332962",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Import the tools we'll use in our RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44b230fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/RAG Course Enhaced/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from pypdf import PdfReader\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e792e43a",
   "metadata": {},
   "source": [
    "## Initial Configuration\n",
    "\n",
    "Configure LM Studio as our local LLM provider. \n",
    "\n",
    "**Important**: Make sure LM Studio is running with a model loaded and the local server enabled (default: `http://localhost:1234`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ba60407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì LM Studio endpoint configured\n",
      "  URL: http://127.0.0.1:1234/v1/chat/completions\n",
      "‚úì Embedding model loaded\n"
     ]
    }
   ],
   "source": [
    "# LM Studio Configuration\n",
    "# LM Studio provides an OpenAI-compatible REST API running locally\n",
    "LM_STUDIO_URL = \"http://127.0.0.1:1234/v1/chat/completions\"\n",
    "\n",
    "def chat_with_llm(messages, max_tokens=256, temperature=0.3):\n",
    "    \"\"\"Send messages to LM Studio and get a response.\"\"\"\n",
    "    payload = {\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    \n",
    "    response = requests.post(LM_STUDIO_URL, json=payload)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "# Model for generating embeddings (runs locally, no API needed)\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(\"‚úì LM Studio endpoint configured\")\n",
    "print(f\"  URL: {LM_STUDIO_URL}\")\n",
    "print(\"‚úì Embedding model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8125625d",
   "metadata": {},
   "source": [
    "## Test LM Studio Connection\n",
    "\n",
    "Let's verify that LM Studio is running and responding correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d76930bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì LM Studio connection successful!\n",
      "  Response: OK\n"
     ]
    }
   ],
   "source": [
    "# Test connection to LM Studio\n",
    "try:\n",
    "    test_response = chat_with_llm([{\"role\": \"user\", \"content\": \"Say 'OK' if you can hear me.\"}], max_tokens=10)\n",
    "    print(\"‚úì LM Studio connection successful!\")\n",
    "    print(f\"  Response: {test_response}\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"‚úó Could not connect to LM Studio\")\n",
    "    print(\"\\n  Make sure:\")\n",
    "    print(\"  1. LM Studio is running\")\n",
    "    print(\"  2. A model is loaded\")\n",
    "    print(\"  3. Local server is enabled (Settings ‚Üí Local Server ‚Üí Start Server)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53742af",
   "metadata": {},
   "source": [
    "## Configuration Parameters\n",
    "\n",
    "Configure chunking and retrieval parameters. Adjust these values to experiment with different settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44a96fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Configuration loaded\n",
      "  Chunk size: 500 chars\n",
      "  Chunk overlap: 50 chars\n",
      "  Min chunk length: 50 chars\n",
      "  Results per query: 3\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Adjust these parameters to experiment!\n",
    "# =============================================================================\n",
    "\n",
    "# Chunking parameters\n",
    "CHUNK_SIZE = 500          # Maximum characters per chunk\n",
    "CHUNK_OVERLAP = 50        # Characters to overlap between chunks\n",
    "MIN_CHUNK_LENGTH = 50     # Minimum characters to keep a chunk (filters noise)\n",
    "\n",
    "# Retrieval parameters\n",
    "NUM_RESULTS = 3           # Number of documents to retrieve per query\n",
    "\n",
    "# Document paths\n",
    "DOCS_PATH = \"Docs\"\n",
    "HR_PDF = \"Company Policies.pdf\"      # HR knowledge base\n",
    "TECH_PDF = \"Corporate Policies.pdf\"  # TECH knowledge base\n",
    "\n",
    "# Metadata to extract (set to True/False)\n",
    "METADATA_OPTIONS = {\n",
    "    \"source\": True,       # Source filename\n",
    "    \"page\": True,         # Page number\n",
    "    \"chunk_id\": True,     # Chunk identifier within document\n",
    "    \"char_count\": True,   # Character count of chunk\n",
    "    \"category\": True,     # HR or TECH category\n",
    "}\n",
    "\n",
    "print(\"‚úì Configuration loaded\")\n",
    "print(f\"  Chunk size: {CHUNK_SIZE} chars\")\n",
    "print(f\"  Chunk overlap: {CHUNK_OVERLAP} chars\")\n",
    "print(f\"  Min chunk length: {MIN_CHUNK_LENGTH} chars\")\n",
    "print(f\"  Results per query: {NUM_RESULTS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f28536",
   "metadata": {},
   "source": [
    "## Load Knowledge Bases from PDFs\n",
    "\n",
    "Load documents from the PDF files with configurable chunking:\n",
    "- **Company Policies.pdf** ‚Üí HR knowledge base\n",
    "- **Corporate Policies.pdf** ‚Üí TECH knowledge base\n",
    "\n",
    "The chunking algorithm:\n",
    "1. Splits text into chunks of `CHUNK_SIZE` characters\n",
    "2. Tries to break at sentence boundaries (periods, newlines)\n",
    "3. Overlaps chunks by `CHUNK_OVERLAP` characters for context continuity\n",
    "4. Filters out chunks smaller than `MIN_CHUNK_LENGTH`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71dc03d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 34 chunks from Company Policies.pdf (HR)\n",
      "‚úì Loaded 31 chunks from Corporate Policies.pdf (TECH)\n",
      "‚úì Loaded 1 chunks for General queries\n",
      "\n",
      "üìÑ Sample chunk from HR:\n",
      "   Length: 352 chars\n",
      "   Page: 1\n",
      "   Preview: COMPANY POLICIES \n",
      "Employee Handbook \n",
      "TABLE OF CONTENTS \n",
      "1. Introduction and Purpose \n",
      "2. Code of Cond...\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):\n",
    "    \"\"\"Split text into overlapping chunks of specified size.\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        \n",
    "        # Try to break at sentence end if possible\n",
    "        if end < len(text):\n",
    "            last_period = chunk.rfind('.')\n",
    "            last_newline = chunk.rfind('\\n')\n",
    "            break_point = max(last_period, last_newline)\n",
    "            if break_point > chunk_size // 2:  # Only if we found a good break point\n",
    "                chunk = chunk[:break_point + 1]\n",
    "                end = start + break_point + 1\n",
    "        \n",
    "        chunks.append(chunk.strip())\n",
    "        start = end - overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def load_pdf(filepath, category):\n",
    "    \"\"\"Extract text from a PDF file and split into configurable chunks.\"\"\"\n",
    "    reader = PdfReader(filepath)\n",
    "    all_chunks = []\n",
    "    chunk_counter = 0\n",
    "    \n",
    "    for page_num, page in enumerate(reader.pages, 1):\n",
    "        text = page.extract_text()\n",
    "        if text:\n",
    "            # Clean up the text\n",
    "            text = text.replace('\\x00', '')  # Remove null characters\n",
    "            \n",
    "            # Split into chunks\n",
    "            page_chunks = chunk_text(text)\n",
    "            \n",
    "            for chunk in page_chunks:\n",
    "                # Skip very short chunks\n",
    "                if len(chunk) < MIN_CHUNK_LENGTH:\n",
    "                    continue\n",
    "                \n",
    "                chunk_counter += 1\n",
    "                \n",
    "                # Build metadata based on configuration\n",
    "                metadata = {}\n",
    "                if METADATA_OPTIONS.get(\"source\"):\n",
    "                    metadata[\"source\"] = os.path.basename(filepath)\n",
    "                if METADATA_OPTIONS.get(\"page\"):\n",
    "                    metadata[\"page\"] = page_num\n",
    "                if METADATA_OPTIONS.get(\"chunk_id\"):\n",
    "                    metadata[\"chunk_id\"] = chunk_counter\n",
    "                if METADATA_OPTIONS.get(\"char_count\"):\n",
    "                    metadata[\"char_count\"] = len(chunk)\n",
    "                if METADATA_OPTIONS.get(\"category\"):\n",
    "                    metadata[\"category\"] = category\n",
    "                \n",
    "                all_chunks.append({\n",
    "                    \"text\": chunk,\n",
    "                    \"metadata\": metadata\n",
    "                })\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "# Load HR documents (Company Policies)\n",
    "hr_pdf_path = os.path.join(DOCS_PATH, HR_PDF)\n",
    "hr_chunks = load_pdf(hr_pdf_path, category=\"HR\")\n",
    "print(f\"‚úì Loaded {len(hr_chunks)} chunks from {HR_PDF} (HR)\")\n",
    "\n",
    "# Load TECH documents (Corporate Policies)  \n",
    "tech_pdf_path = os.path.join(DOCS_PATH, TECH_PDF)\n",
    "tech_chunks = load_pdf(tech_pdf_path, category=\"TECH\")\n",
    "print(f\"‚úì Loaded {len(tech_chunks)} chunks from {TECH_PDF} (TECH)\")\n",
    "\n",
    "# General documents (for OTHER intent)\n",
    "general_chunks = [{\n",
    "    \"text\": \"For questions not related to HR or IT policies, please contact your manager or the reception desk.\",\n",
    "    \"metadata\": {\"source\": \"General\", \"page\": 0, \"chunk_id\": 1, \"char_count\": 97, \"category\": \"OTHER\"}\n",
    "}]\n",
    "print(f\"‚úì Loaded {len(general_chunks)} chunks for General queries\")\n",
    "\n",
    "# Show sample chunk\n",
    "print(f\"\\nüìÑ Sample chunk from HR:\")\n",
    "print(f\"   Length: {hr_chunks[0]['metadata'].get('char_count', 'N/A')} chars\")\n",
    "print(f\"   Page: {hr_chunks[0]['metadata'].get('page', 'N/A')}\")\n",
    "print(f\"   Preview: {hr_chunks[0]['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c363b87a",
   "metadata": {},
   "source": [
    "## Initialize Vector Database\n",
    "\n",
    "Use ChromaDB to store and search documents using embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1935ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Vector database initialized\n",
      "  Collections: hr_base, tech_base, general_base\n"
     ]
    }
   ],
   "source": [
    "# Initialize ChromaDB in memory\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# Create collections for each knowledge base\n",
    "# Note: ChromaDB requires collection names to be at least 3 characters\n",
    "hr_collection = chroma_client.get_or_create_collection(name=\"hr_base\")\n",
    "tech_collection = chroma_client.get_or_create_collection(name=\"tech_base\")\n",
    "general_collection = chroma_client.get_or_create_collection(name=\"general_base\")\n",
    "\n",
    "print(\"‚úì Vector database initialized\")\n",
    "print(\"  Collections: hr_base, tech_base, general_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617c77a9",
   "metadata": {},
   "source": [
    "## Load Documents into Vector Database\n",
    "\n",
    "Convert document chunks to embeddings and store them in their respective collections with metadata (source file and page number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4088beb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings (this may take a moment)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 34 chunks into HR collection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 31 chunks into TECH collection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 58.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 1 chunks into GENERAL collection\n",
      "\n",
      "‚úì All collections loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def load_chunks_to_collection(collection, chunks, prefix):\n",
    "    \"\"\"Load document chunks into a collection with embeddings and metadata.\"\"\"\n",
    "    if not chunks:\n",
    "        print(f\"‚ö† No chunks to load for {prefix.upper()}\")\n",
    "        return\n",
    "    \n",
    "    texts = [chunk[\"text\"] for chunk in chunks]\n",
    "    embeddings = embedding_model.encode(texts, show_progress_bar=True)\n",
    "    \n",
    "    for i, (chunk, emb) in enumerate(zip(chunks, embeddings)):\n",
    "        collection.add(\n",
    "            embeddings=[emb.tolist()],\n",
    "            documents=[chunk[\"text\"]],\n",
    "            metadatas=[chunk[\"metadata\"]],\n",
    "            ids=[f\"{prefix}_{i}\"]\n",
    "        )\n",
    "    print(f\"‚úì Loaded {len(chunks)} chunks into {prefix.upper()} collection\")\n",
    "\n",
    "# Load all knowledge bases\n",
    "print(\"Loading embeddings (this may take a moment)...\")\n",
    "load_chunks_to_collection(hr_collection, hr_chunks, \"hr\")\n",
    "load_chunks_to_collection(tech_collection, tech_chunks, \"tech\")\n",
    "load_chunks_to_collection(general_collection, general_chunks, \"general\")\n",
    "print(\"\\n‚úì All collections loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5432f1",
   "metadata": {},
   "source": [
    "## Step 1: Intent Classifier\n",
    "\n",
    "Use an LLM to classify the user's question into one of three categories:\n",
    "\n",
    "- **HR**: Questions about employees, attendance, leave, benefits, dress code, workplace behavior, harassment, conflicts, disciplinary procedures, grievances, or the employee handbook.\n",
    "\n",
    "- **TECH**: Questions about data protection, GDPR/CCPA, information security, acceptable use of technology, passwords, MFA, VPN, incident response, data retention, or remote work security.\n",
    "\n",
    "- **OTHER**: Questions not related to HR policies or data privacy/technology policies, or that are too general or unclear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4145a13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intent Classifier Prompt (System + User)\n",
    "CLASSIFIER_SYSTEM_PROMPT = \"\"\"You are an intent classifier for a corporate assistant. Your ONLY job is to classify questions into exactly one category.\n",
    "\n",
    "Categories:\n",
    "- HR: Questions about employees, attendance, leave, benefits, dress code, workplace behavior, \n",
    "  harassment, conflicts, disciplinary procedures, grievances, or the employee handbook.\n",
    "- TECH: Questions about data protection, GDPR/CCPA, information security, acceptable use of technology, \n",
    "  passwords, MFA, VPN, incident response, data retention, or remote work security.\n",
    "- OTHER: Questions not related to HR policies or data privacy/technology policies, or that are too general or unclear.\n",
    "\n",
    "Rules:\n",
    "- Respond with ONLY one word: HR, TECH, or OTHER\n",
    "- Do not explain your reasoning\n",
    "- Do not add any other text\"\"\"\n",
    "\n",
    "def classify_intent(question):\n",
    "    \"\"\"Classify the user's question intent using local LLM.\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": CLASSIFIER_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": f\"Classify this question: {question}\"}\n",
    "    ]\n",
    "    \n",
    "    intent = chat_with_llm(messages, max_tokens=10, temperature=0).upper()\n",
    "    \n",
    "    # Ensure we get a valid intent\n",
    "    if \"HR\" in intent:\n",
    "        return \"HR\"\n",
    "    elif \"TECH\" in intent:\n",
    "        return \"TECH\"\n",
    "    else:\n",
    "        return \"OTHER\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62bd741",
   "metadata": {},
   "source": [
    "## Step 2: Router (Switch)\n",
    "\n",
    "Based on the classified intent, select the correct knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71be19e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_collection(intent):\n",
    "    \"\"\"Route the query to the appropriate collection based on intent.\"\"\"\n",
    "    \n",
    "    if intent == \"HR\":\n",
    "        return hr_collection, \"HR Base\"\n",
    "    elif intent == \"TECH\":\n",
    "        return tech_collection, \"IT Support Base\"\n",
    "    else:\n",
    "        return general_collection, \"General Base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1623c31",
   "metadata": {},
   "source": [
    "## Step 3: Retrieval (Document Search)\n",
    "\n",
    "Search for the most relevant documents in the selected knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9466a1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_relevant_documents(collection, question, n_results=NUM_RESULTS):\n",
    "    \"\"\"Search for relevant documents using embeddings.\"\"\"\n",
    "    \n",
    "    # Generate question embedding\n",
    "    question_embedding = embedding_model.encode([question])[0]\n",
    "    \n",
    "    # Search for similar documents\n",
    "    results = collection.query(\n",
    "        query_embeddings=[question_embedding.tolist()],\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    # Return documents with their metadata and similarity score\n",
    "    docs_with_meta = []\n",
    "    for doc, meta, dist in zip(results['documents'][0], results['metadatas'][0], results['distances'][0]):\n",
    "        docs_with_meta.append({\n",
    "            \"text\": doc,\n",
    "            \"source\": meta.get(\"source\", \"Unknown\"),\n",
    "            \"page\": meta.get(\"page\", 0),\n",
    "            \"chunk_id\": meta.get(\"chunk_id\", 0),\n",
    "            \"char_count\": meta.get(\"char_count\", len(doc)),\n",
    "            \"category\": meta.get(\"category\", \"Unknown\"),\n",
    "            \"similarity\": round(1 - dist, 3)  # Convert distance to similarity\n",
    "        })\n",
    "    \n",
    "    return docs_with_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199ff501",
   "metadata": {},
   "source": [
    "## Step 4: Response Generator\n",
    "\n",
    "Use an LLM to generate a response based on the retrieved context.\n",
    "\n",
    "The generator uses a specialized prompt that:\n",
    "- Acts as the Official HR & IT Assistant\n",
    "- Answers based ONLY on the provided context\n",
    "- Cites sources at the end of the answer\n",
    "- Refuses to invent information\n",
    "- Follows safety guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d3317d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response Generator System Prompt\n",
    "GENERATOR_SYSTEM_PROMPT = \"\"\"<role>\n",
    "You are the Official HR & IT Assistant for the company. Your goal is to answer employee questions ACCURATELY based\n",
    "ONLY on the provided context. You represent the company, so be professional, polite, and concise.\n",
    "</role>\n",
    "\n",
    "<instructions>\n",
    "1. **Analyze the Context:** Read the provided document chunks carefully.\n",
    "\n",
    "2. **Direct Answer:** Answer the user's question directly. Do not start with \"Based on the documents...\". Just say the answer.\n",
    "\n",
    "3. **Cite Your Source:** At the end of your answer, mention which policy or section supports your statement (\n",
    "    e.g., \"Source: Company Policies, Page 3\").\n",
    "\n",
    "4. **Be Honest:** If the answer is NOT in the provided context, state clearly: \"I cannot find specific information about that in \n",
    "    the current policies. Please check with HR directly.\" DO NOT invent information.\n",
    "\n",
    "5. **Formatting:** Use bullet points for lists (like requirements or steps) to make it easy to read.\n",
    "\n",
    "6. **Tone:** Helpful, clear, and safe.\n",
    "</instructions>\n",
    "\n",
    "<safety_check>\n",
    "- Do not provide medical or legal advice.\n",
    "- Do not share passwords or sensitive keys if they appear in the text.\n",
    "- If the user asks something unethical (how to bypass security), politely refuse based on the \"Acceptable Use Policy\".\n",
    "</safety_check>\"\"\"\n",
    "\n",
    "def generate_response(question, context_docs, intent):\n",
    "    \"\"\"Generate a response using the local LLM with retrieved context.\"\"\"\n",
    "    \n",
    "    # Format context with source citations\n",
    "    context_parts = []\n",
    "    for doc in context_docs:\n",
    "        source_info = f\"[{doc['source']}, Page {doc['page']}]\"\n",
    "        context_parts.append(f\"{source_info}\\n{doc['text']}\")\n",
    "    \n",
    "    context_text = \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # Customize context header based on intent\n",
    "    if intent == \"HR\":\n",
    "        context_header = \"Relevant excerpts from the Employee Handbook (Company Policies):\"\n",
    "    elif intent == \"TECH\":\n",
    "        context_header = \"Relevant excerpts from Tech/Security Policies (Corporate Policies):\"\n",
    "    else:\n",
    "        context_header = \"Relevant company information:\"\n",
    "    \n",
    "    user_message = f\"\"\"<context>\n",
    "{context_header}\n",
    "\n",
    "{context_text}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": GENERATOR_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    \n",
    "    return chat_with_llm(messages, max_tokens=512, temperature=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1e2f8e",
   "metadata": {},
   "source": [
    "## Complete RAG Pipeline\n",
    "\n",
    "Combine all steps into a function that executes the complete flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e9586fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(question):\n",
    "    \"\"\"Execute the complete RAG pipeline.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"üìù QUESTION: {question}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Step 1: Classify intent\n",
    "    print(\"\\nüîç Step 1: Classifying intent...\")\n",
    "    intent = classify_intent(question)\n",
    "    print(f\"   ‚Üí Detected intent: {intent}\")\n",
    "    \n",
    "    # Step 2: Route to the correct collection\n",
    "    print(\"\\nüîÄ Step 2: Routing to knowledge base...\")\n",
    "    collection, base_name = route_collection(intent)\n",
    "    print(f\"   ‚Üí Selected base: {base_name}\")\n",
    "    \n",
    "    # Step 3: Search for relevant documents\n",
    "    print(\"\\nüìö Step 3: Retrieving relevant documents...\")\n",
    "    documents = search_relevant_documents(collection, question)\n",
    "    for i, doc in enumerate(documents, 1):\n",
    "        print(f\"   {i}. [{doc['source']}, p.{doc['page']}, chunk #{doc['chunk_id']}]\")\n",
    "        print(f\"      Similarity: {doc['similarity']} | Chars: {doc['char_count']}\")\n",
    "        print(f\"      Preview: {doc['text'][:60]}...\")\n",
    "    \n",
    "    # Step 4: Generate response\n",
    "    print(\"\\nü§ñ Step 4: Generating response with LLM...\")\n",
    "    response = generate_response(question, documents, intent)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ FINAL ANSWER:\")\n",
    "    print(response)\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9c53c8",
   "metadata": {},
   "source": [
    "## Usage Examples\n",
    "\n",
    "Let's test our RAG with different types of questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eb3135",
   "metadata": {},
   "source": [
    "### Example 1: HR Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d9d73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipeline(\"How many vacation days do I have per year?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5509a7b1",
   "metadata": {},
   "source": [
    "### Example 2: IT Support Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba23824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipeline(\"How do I reset my corporate password?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c934f85e",
   "metadata": {},
   "source": [
    "### Example 3: General Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d086fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipeline(\"Where is the cafeteria?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34066e2c",
   "metadata": {},
   "source": [
    "## Test Your Own Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14dfa504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìù QUESTION: Can I work from home?\n",
      "============================================================\n",
      "\n",
      "üîç Step 1: Classifying intent...\n",
      "   ‚Üí Detected intent: OTHER\n",
      "\n",
      "üîÄ Step 2: Routing to knowledge base...\n",
      "   ‚Üí Selected base: General Base\n",
      "\n",
      "üìö Step 3: Retrieving relevant documents...\n",
      "   1. [General, p.0, chunk #1]\n",
      "      Similarity: -0.487 | Chars: 97\n",
      "      Preview: For questions not related to HR or IT policies, please conta...\n",
      "\n",
      "ü§ñ Step 4: Generating response with LLM...\n",
      "\n",
      "============================================================\n",
      "‚úÖ FINAL ANSWER:\n",
      "* Direct Answer: Yes, you can work from home with prior approval from your manager and HR. (Source: Company Policies, Page 2)\n",
      "* Cite Your Source: Company Policies, Page 2\n",
      "* Be Honest: If the answer is NOT in the provided context, state clearly: I cannot find specific information about that in the current policies. Please check with HR directly.\n",
      "* Formatting: Bullet points for lists (like requirements or steps) to make it easy to read.\n",
      "* Tone: Helpful, clear, and safe.\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'* Direct Answer: Yes, you can work from home with prior approval from your manager and HR. (Source: Company Policies, Page 2)\\n* Cite Your Source: Company Policies, Page 2\\n* Be Honest: If the answer is NOT in the provided context, state clearly: I cannot find specific information about that in the current policies. Please check with HR directly.\\n* Formatting: Bullet points for lists (like requirements or steps) to make it easy to read.\\n* Tone: Helpful, clear, and safe.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the question to whatever you want to test\n",
    "my_question = \"Can I work from home?\"\n",
    "rag_pipeline(my_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1406ec28-15d1-4d09-a21c-4e147834c894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries for a simple local RAG setup\n",
    "# - langchain, langchain-community, langchain-ollama: LangChain core and community integrations (ollama helps connect to a local LLM)\n",
    "# - chromadb: vector store for embeddings\n",
    "# - pypdf: PDF document loading\n",
    "# - sentence-transformers: embeddings models\n",
    "# - directory-loader: helper for loading documents from a directory\n",
    "# Install quietly (-q). Use %pip (recommended within Jupyter) rather than !pip.\n",
    "%pip install -q --upgrade pip\n",
    "%pip install -q langchain langchain-community langchain-ollama chromadb pypdf sentence-transformers directory-loader\n",
    "\n",
    "# Try to restart the kernel automatically so newly installed packages can be imported.\n",
    "# Automatic restart works in classic Jupyter; if it fails, please restart the kernel manually (Kernel -> Restart).\n",
    "from IPython.display import display, Javascript\n",
    "try:\n",
    "    display(Javascript(\"Jupyter.notebook.kernel.restart()\"))\n",
    "except Exception:\n",
    "    try:\n",
    "        display(Javascript(\"IPython.notebook.kernel.restart()\"))\n",
    "    except Exception:\n",
    "        print(\"Install complete. Please restart the kernel manually (Kernel -> Restart) to use the new packages.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55789ba",
   "metadata": {},
   "source": [
    "## RAG Flow Summary\n",
    "\n",
    "Our system implements the following flow:\n",
    "\n",
    "```\n",
    "1. User asks a question\n",
    "   ‚Üì\n",
    "2. LLM Classifier (LM Studio) ‚Üí identifies intent (HR/TECH/OTHER)\n",
    "   ‚Üì\n",
    "3. Switch/Router ‚Üí selects knowledge base\n",
    "   ‚Üì\n",
    "4. Retrieval ‚Üí searches relevant documents using embeddings\n",
    "   ‚Üì\n",
    "5. Generator (LM Studio) ‚Üí LLM creates response with context\n",
    "   ‚Üì\n",
    "6. User receives grounded answer\n",
    "```\n",
    "\n",
    "### Components used:\n",
    "- üñ•Ô∏è **LM Studio**: Local LLM for classification and generation (privacy-friendly!)\n",
    "- üî¢ **Sentence Transformers**: Local embeddings (all-MiniLM-L6-v2)\n",
    "- üì¶ **ChromaDB**: In-memory vector database\n",
    "- üåê **Requests**: Simple HTTP client (works globally, no restrictions)\n",
    "\n",
    "### Advantages of this approach:\n",
    "- ‚úÖ **100% Local**: No data leaves your machine\n",
    "- ‚úÖ **No API costs**: Everything runs locally\n",
    "- ‚úÖ **Global availability**: Uses standard `requests` library (works in China, etc.)\n",
    "- ‚úÖ **Grounded responses**: Based on real documents\n",
    "- ‚úÖ **Targeted search**: Each question goes to the right base\n",
    "- ‚úÖ **Traceability**: We can see which documents were used\n",
    "\n",
    "### Possible improvements:\n",
    "- üìä Add confidence metrics\n",
    "- üîÑ Implement document re-ranking\n",
    "- üìù Load documents from real PDFs\n",
    "- üéØ Improve classifier with few-shot examples\n",
    "- üíæ Use a persistent database"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
