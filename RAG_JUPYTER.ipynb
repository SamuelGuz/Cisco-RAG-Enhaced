{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "045af654",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Step 1: Install Required Libraries\n",
    "\n",
    "### What Does This Code Do?\n",
    "Installs all the Python libraries we need to build our RAG system.\n",
    "\n",
    "### ü§ñ How to Ask the Jupyter Agent:\n",
    "```\n",
    "/generate Install the libraries I need to build a RAG system: langchain, chromadb, pypdf, openai, and sentence-transformers\n",
    "```\n",
    "\n",
    "### Explanation of Each Library:\n",
    "| Library | What Is It For? |\n",
    "|---------|----------------|\n",
    "| `langchain` | Main framework for working with LLMs |\n",
    "| `langchain-community` | Connectors for ChromaDB and other services |\n",
    "| `chromadb` | Vector database (stores the embeddings) |\n",
    "| `pypdf` | Read PDF files |\n",
    "| `openai` | Client to connect with LM Studio |\n",
    "| `sentence-transformers` | Embedding models |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58c4be4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required libraries\n",
    "# The -q flag means \"quiet\" (less output)\n",
    "\n",
    "!pip install -q langchain langchain-community chromadb pypdf openai sentence-transformers tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e2bad0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìÑ Step 2: Load the PDF Document\n",
    "\n",
    "### What Does This Code Do?\n",
    "Reads a PDF file and extracts all the text it contains. Each page becomes a separate \"document\".\n",
    "\n",
    "### ü§ñ How to Ask the Jupyter Agent:\n",
    "```\n",
    "/generate Load a PDF file called \"Company Policies.pdf\" that is in the Docs folder\n",
    "```\n",
    "\n",
    "### Key Concepts:\n",
    "- **Document Loader**: Tool that reads files and converts them into objects that LangChain can use\n",
    "- **docs**: List of documents, where each document contains the content of one page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a31722a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 8 pages from the PDF\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "# First, let's check where we are\n",
    "print(f\"üìÇ Current directory: {os.getcwd()}\")\n",
    "\n",
    "# Use absolute path to be safe\n",
    "file_name = \"/home/user/RAG Course Enhaced/Docs/Company Policies.pdf\"\n",
    "\n",
    "# Verify the file exists\n",
    "if os.path.exists(file_name):\n",
    "    print(f\"‚úÖ File found: {file_name}\")\n",
    "else:\n",
    "    print(f\"‚ùå File not found: {file_name}\")\n",
    "    print(f\"Available files in Docs folder:\")\n",
    "    docs_folder = \"/home/user/RAG Course Enhaced/Docs\"\n",
    "    if os.path.exists(docs_folder):\n",
    "        for f in os.listdir(docs_folder):\n",
    "            print(f\"  - {f}\")\n",
    "\n",
    "# Create the PDF loader\n",
    "loader = PyPDFLoader(file_name)\n",
    "\n",
    "# Load the document (this reads all pages)\n",
    "docs = loader.load()\n",
    "\n",
    "# Show how many pages were loaded\n",
    "print(f\"‚úÖ Loaded {len(docs)} pages from the PDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05406f65",
   "metadata": {},
   "source": [
    "### üîç Let's Explore What a Document Contains\n",
    "\n",
    "Let's look at the first page to understand the structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31d035db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "CONTENT OF THE FIRST PAGE:\n",
      "==================================================\n",
      "COMPANY POLICIES \n",
      "Employee Handbook \n",
      "TABLE OF CONTENTS \n",
      "1. Introduction and Purpose \n",
      "2. Code of Conduct \n",
      "3. Attendance and Punctuality \n",
      "4. Leave Policy \n",
      "5. Workplace Health and Safety \n",
      "6. Anti-Harassment and Non-Discrimination \n",
      "7. Dress Code \n",
      "8. Conflict of Interest \n",
      "9. Disciplinary Procedures \n",
      "10. Grievance Procedures \n",
      "11. Employee Benefits Overview\n",
      "\n",
      "...\n",
      "\n",
      "üìã Metadata: {'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-12-17T10:11:12-08:00', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'moddate': '2025-12-17T10:11:12-08:00', 'source': './Docs/Company Policies.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "# Let's see the content of the first page\n",
    "print(\"=\" * 50)\n",
    "print(\"CONTENT OF THE FIRST PAGE:\")\n",
    "print(\"=\" * 50)\n",
    "print(docs[0].page_content[:500])  # First 500 characters\n",
    "print(\"\\n...\")\n",
    "print(\"\\nüìã Metadata:\", docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf1bf2e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÇÔ∏è Step 3: Split the Text into Chunks\n",
    "\n",
    "### Why Do We Need to Split the Text?\n",
    "\n",
    "Embedding models have a **token limit** they can process. If we give them a very long text, they'll only process part of it and we'll lose information.\n",
    "\n",
    "### What Is a \"Chunk\"?\n",
    "A chunk is a fragment of the original document. We split the text into smaller chunks to:\n",
    "1. Fit within the embedding model's limit\n",
    "2. Make searches more precise (find exactly the relevant part)\n",
    "\n",
    "### ü§ñ How to Ask the Jupyter Agent:\n",
    "```\n",
    "/generate Split the docs variable into smaller chunks using langchain. Use 500 characters per chunk with 50 character overlap.\n",
    "```\n",
    "\n",
    "### Important Parameters:\n",
    "| Parameter | Value | Explanation |\n",
    "|-----------|-------|-------------|\n",
    "| `chunk_size` | 500 | Maximum characters per chunk |\n",
    "| `chunk_overlap` | 50 | Characters that repeat between consecutive chunks |\n",
    "\n",
    "### Why Do We Use Overlap?\n",
    "Overlap ensures we don't cut ideas in half. If an important sentence is between two chunks, the overlap makes it appear complete in at least one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ccb7c683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created 32 chunks from 8 pages\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create the text splitter\n",
    "# chunk_size=500: each fragment will have a maximum of 500 characters\n",
    "# chunk_overlap=50: fragments overlap by 50 characters\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "# Split the documents into chunks\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# Show how many chunks were created\n",
    "print(f\"‚úÖ Created {len(chunks)} chunks from {len(docs)} pages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2414d65c",
   "metadata": {},
   "source": [
    "### üîç Let's See Some Example Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b62501b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST CHUNK:\n",
      "----------------------------------------\n",
      "COMPANY POLICIES \n",
      "Employee Handbook \n",
      "TABLE OF CONTENTS \n",
      "1. Introduction and Purpose \n",
      "2. Code of Conduct \n",
      "3. Attendance and Punctuality \n",
      "4. Leave Policy \n",
      "5. Workplace Health and Safety \n",
      "6. Anti-Harassment and Non-Discrimination \n",
      "7. Dress Code \n",
      "8. Conflict of Interest \n",
      "9. Disciplinary Procedures \n",
      "10. Grievance Procedures \n",
      "11. Employee Benefits Overview\n",
      "\n",
      "üìè Length: 352 characters\n"
     ]
    }
   ],
   "source": [
    "# Let's see the first chunk\n",
    "print(\"FIRST CHUNK:\")\n",
    "print(\"-\" * 40)\n",
    "print(chunks[0].page_content)\n",
    "print(f\"\\nüìè Length: {len(chunks[0].page_content)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf7bd56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAST CHUNK:\n",
      "----------------------------------------\n",
      "company property may result in deductions from final pay as permitted by law. \n",
      "11.8 Final Pay \n",
      "Final pay will be processed in accordance with applicable state and federal laws. This includes \n",
      "payment for all hours worked, accrued but unused vacation time as applicable, and any other earned \n",
      "compensation. Information about benefit continuation options will be provided.\n",
      "\n",
      "üìè Length: 370 characters\n"
     ]
    }
   ],
   "source": [
    "# Let's see the last chunk\n",
    "print(\"LAST CHUNK:\")\n",
    "print(\"-\" * 40)\n",
    "print(chunks[-1].page_content)\n",
    "print(f\"\\nüìè Length: {len(chunks[-1].page_content)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdf98e6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßÆ Step 4: Create the Embedding Function\n",
    "\n",
    "### What Are Embeddings?\n",
    "\n",
    "**Embeddings** are numerical representations of text. They convert words and sentences into vectors (lists of numbers) that capture semantic meaning.\n",
    "\n",
    "**Simple Example:**\n",
    "- \"dog\" and \"puppy\" will have similar embeddings (they're close in vector space)\n",
    "- \"dog\" and \"mathematics\" will have very different embeddings (they're far apart)\n",
    "\n",
    "### What Model Will We Use?\n",
    "\n",
    "We'll use **nomic-embed-text:v1.5** through LM Studio. This model:\n",
    "- Is open source and free\n",
    "- Has good performance for English and Spanish texts\n",
    "- Can process up to 8192 tokens\n",
    "\n",
    "### ü§ñ How to Ask the Jupyter Agent:\n",
    "```\n",
    "/generate Create an embedding function that connects to LM Studio running on localhost port 1234. Use the nomic-embed-text model.\n",
    "```\n",
    "\n",
    "### ‚ö†Ô∏è Important:\n",
    "Make sure you have the `nomic-embed-text-v1.5` model loaded in LM Studio before running this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80f456ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embedding function configured successfully\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from typing import List\n",
    "from langchain_core.embeddings import Embeddings\n",
    "\n",
    "class LMStudioEmbeddings(Embeddings):\n",
    "    \"\"\"Custom embedding class for LM Studio compatibility.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str, model: str):\n",
    "        self.base_url = base_url\n",
    "        self.model = model\n",
    "        self.url = f\"{base_url}/embeddings\"\n",
    "    \n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed a list of documents.\"\"\"\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            response = requests.post(\n",
    "                self.url,\n",
    "                json={\"input\": text, \"model\": self.model}\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            embeddings.append(response.json()[\"data\"][0][\"embedding\"])\n",
    "        return embeddings\n",
    "    \n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed a single query.\"\"\"\n",
    "        response = requests.post(\n",
    "            self.url,\n",
    "            json={\"input\": text, \"model\": self.model}\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"data\"][0][\"embedding\"]\n",
    "\n",
    "# Configure the embedding function to connect with LM Studio\n",
    "embedding_function = LMStudioEmbeddings(\n",
    "    base_url=\"http://127.0.0.1:1234/v1\",\n",
    "    model=\"nomic-embed-text-v1.5\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Embedding function configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a174353",
   "metadata": {},
   "source": [
    "### üß™ Let's Test the Embeddings\n",
    "\n",
    "Let's see how embeddings work with a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "566981c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Text: 'I love programming in Python'\n",
      "üìä Embedding dimensions: 768\n",
      "üî¢ First 5 values: [-0.016399774700403214, 0.08437343686819077, -0.12448632717132568, -0.020135633647441864, 0.02330748550593853]\n"
     ]
    }
   ],
   "source": [
    "# Test with a simple sentence\n",
    "test_text = \"I love programming in Python\"\n",
    "\n",
    "# Get the embedding\n",
    "embedding = embedding_function.embed_query(test_text)\n",
    "\n",
    "# Show embedding information\n",
    "print(f\"üìù Text: '{test_text}'\")\n",
    "print(f\"üìä Embedding dimensions: {len(embedding)}\")\n",
    "print(f\"üî¢ First 5 values: {embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dc3b36",
   "metadata": {},
   "source": [
    "### üéØ Let's See the Similarity Between Texts\n",
    "\n",
    "Embeddings allow us to measure how similar two texts are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "abcc07ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Base text: 'I really enjoy coffee in the morning'\n",
      "\n",
      "Similarity with other texts:\n",
      "--------------------------------------------------\n",
      "  'I like drinking coffee when I wake up'\n",
      "  ‚Üí Similarity: 0.8733\n",
      "\n",
      "  'Programming is my passion'\n",
      "  ‚Üí Similarity: 0.5398\n",
      "\n",
      "  'Cats are independent animals'\n",
      "  ‚Üí Similarity: 0.3898\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\n",
    "    Returns a value between -1 and 1, where 1 = identical.\"\"\"\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# Test texts\n",
    "texts = [\n",
    "    \"I really enjoy coffee in the morning\",\n",
    "    \"I like drinking coffee when I wake up\",\n",
    "    \"Programming is my passion\",\n",
    "    \"Cats are independent animals\"\n",
    "]\n",
    "\n",
    "# Get the embeddings\n",
    "embeddings = embedding_function.embed_documents(texts)\n",
    "\n",
    "# Compare the first text with all others\n",
    "print(f\"üìù Base text: '{texts[0]}'\\n\")\n",
    "print(\"Similarity with other texts:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i in range(1, len(texts)):\n",
    "    sim = cosine_similarity(embeddings[0], embeddings[i])\n",
    "    print(f\"  '{texts[i]}'\")\n",
    "    print(f\"  ‚Üí Similarity: {sim:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae3b1cb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üóÑÔ∏è Step 5: Create the Vector Database (ChromaDB)\n",
    "\n",
    "### What Is a Vector Database?\n",
    "\n",
    "A vector database stores embeddings and allows you to quickly search for the most similar ones to a query. It's like a very efficient index for finding similar texts.\n",
    "\n",
    "### What Does ChromaDB Do?\n",
    "1. **Stores** the chunks and their embeddings\n",
    "2. **Indexes** the embeddings for fast searches\n",
    "3. **Searches** for the most similar chunks to a question\n",
    "\n",
    "### ü§ñ How to Ask the Jupyter Agent:\n",
    "```\n",
    "/generate Create a chromadb database using the chunks and embedding_function variables. Save it to a folder called my_database.\n",
    "```\n",
    "\n",
    "### üí° Note:\n",
    "This step may take a few minutes depending on how many chunks you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "658d6a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Creating vector database...\n",
      "   (This may take a few moments)\n",
      "\n",
      "‚úÖ Database created successfully!\n",
      "üìÅ Saved to: ./my_database\n",
      "üìä Total documents indexed: 32\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Directory where the database will be saved\n",
    "db_directory = \"./my_database\"\n",
    "\n",
    "print(\"üîÑ Creating vector database...\")\n",
    "print(\"   (This may take a few moments)\")\n",
    "\n",
    "# Create the vector database\n",
    "# This: 1) generates embeddings for each chunk, 2) saves them in ChromaDB\n",
    "vector_database = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_function,\n",
    "    persist_directory=db_directory\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Database created successfully!\")\n",
    "print(f\"üìÅ Saved to: {db_directory}\")\n",
    "print(f\"üìä Total documents indexed: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf3edd9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîé Step 6: Search for Relevant Documents\n",
    "\n",
    "### How Does the Search Work?\n",
    "\n",
    "When you ask a question:\n",
    "1. The question is converted into an embedding\n",
    "2. ChromaDB searches for chunks whose embeddings are most similar\n",
    "3. It returns the K most relevant chunks\n",
    "\n",
    "### ü§ñ How to Ask the Jupyter Agent:\n",
    "```\n",
    "/generate Search the vector_database for documents related to \"What is the vacation policy?\" and show me the results.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8c0c90be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Question: 'What is the vacation policy?'\n",
      "\n",
      "üìö Found 3 relevant documents:\n",
      "\n",
      "==================================================\n",
      "üìÑ Document 1:\n",
      "==================================================\n",
      "4. LEAVE POLICY \n",
      "4.1 Annual Leave \n",
      "Full-time employees are entitled to paid annual leave based on their length of service. Leave accrual \n",
      "begins from the first day of employment. Employees must submit leave requests through the \n",
      "appropriate system and obtain approval from their supervisor before taking time off. \n",
      "4.2 Sick Leave \n",
      "Sick leave is provided to employees who are unable to work due to illness or injury. A medical certificate\n",
      "\n",
      "==================================================\n",
      "üìÑ Document 2:\n",
      "==================================================\n",
      "4. LEAVE POLICY \n",
      "4.1 Annual Leave \n",
      "Full-time employees are entitled to paid annual leave based on their length of service. Leave accrual \n",
      "begins from the first day of employment. Employees must submit leave requests through the \n",
      "appropriate system and obtain approval from their supervisor before taking time off. \n",
      "4.2 Sick Leave \n",
      "Sick leave is provided to employees who are unable to work due to illness or injury. A medical certificate\n",
      "\n",
      "==================================================\n",
      "üìÑ Document 3:\n",
      "==================================================\n",
      "COMPANY POLICIES \n",
      "Employee Handbook \n",
      "TABLE OF CONTENTS \n",
      "1. Introduction and Purpose \n",
      "2. Code of Conduct \n",
      "3. Attendance and Punctuality \n",
      "4. Leave Policy \n",
      "5. Workplace Health and Safety \n",
      "6. Anti-Harassment and Non-Discrimination \n",
      "7. Dress Code \n",
      "8. Conflict of Interest \n",
      "9. Disciplinary Procedures \n",
      "10. Grievance Procedures \n",
      "11. Employee Benefits Overview\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define our question\n",
    "question = \"What is the vacation policy?\"\n",
    "\n",
    "# Search for the 3 most relevant chunks\n",
    "relevant_documents = vector_database.similarity_search(question, k=3)\n",
    "\n",
    "# Display the results\n",
    "print(f\"üîç Question: '{question}'\")\n",
    "print(f\"\\nüìö Found {len(relevant_documents)} relevant documents:\\n\")\n",
    "\n",
    "for i, doc in enumerate(relevant_documents, 1):\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"üìÑ Document {i}:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(doc.page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c22dc47",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîå Step 7: Connect to the LLM in LM Studio\n",
    "\n",
    "### What Does This Step Do?\n",
    "\n",
    "We create a client to communicate with the language model running in LM Studio. We use the `openai` library because LM Studio exposes a compatible API.\n",
    "\n",
    "### ü§ñ How to Ask the Jupyter Agent:\n",
    "```\n",
    "/generate Connect to the LLM running in LM Studio on localhost port 1234. Test it with a simple question.\n",
    "```\n",
    "\n",
    "### ‚ö†Ô∏è Important:\n",
    "Make sure LM Studio is running and has a model loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "25d7ae1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM client configured\n",
      "üîó Connected to: http://127.0.0.1:1234\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Configure the client to connect with LM Studio\n",
    "llm_client = OpenAI(\n",
    "    base_url=\"http://127.0.0.1:1234/v1\",  # LM Studio URL\n",
    "    api_key=\"lm-studio\"  # No real API key required\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LLM client configured\")\n",
    "print(\"üîó Connected to: http://127.0.0.1:1234\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc524d75",
   "metadata": {},
   "source": [
    "### üß™ Let's Test the Connection with a Simple Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "03b170d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connection successful!\n",
      "\n",
      "ü§ñ Response: Python is a high-level, interpreted programming language for general-purpose computing, guiding the development of efficient and readable code.\n"
     ]
    }
   ],
   "source": [
    "# Test with a simple question\n",
    "test_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer briefly.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is Python in one sentence?\"}\n",
    "]\n",
    "\n",
    "try:\n",
    "    response = llm_client.chat.completions.create(\n",
    "        model=\"lm-studio\",  # LM Studio uses whatever model is loaded\n",
    "        messages=test_messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Connection successful!\")\n",
    "    print(f\"\\nü§ñ Response: {response.choices[0].message.content}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection error: {e}\")\n",
    "    print(\"\\nüí° Make sure LM Studio is running and has a model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe6bc5a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Step 8: Create the Augmented Prompt\n",
    "\n",
    "### What Is an Augmented Prompt?\n",
    "\n",
    "It's the combination of:\n",
    "1. **Context**: The relevant documents we found\n",
    "2. **Instructions**: How we want the model to respond\n",
    "3. **Question**: What the user wants to know\n",
    "\n",
    "This is the **\"A\" in RAG** - the **Augmentation**.\n",
    "\n",
    "### ü§ñ How to Ask the Jupyter Agent:\n",
    "```\n",
    "/generate Create a function that combines a question with the retrieved documents into a prompt for the LLM.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23b7516c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ create_augmented_prompt function defined\n"
     ]
    }
   ],
   "source": [
    "def create_augmented_prompt(question, documents):\n",
    "    \"\"\"\n",
    "    Creates an augmented prompt by combining the question with context.\n",
    "    \n",
    "    Args:\n",
    "        question: The user's question\n",
    "        documents: List of relevant documents from ChromaDB\n",
    "    \n",
    "    Returns:\n",
    "        String with the complete prompt to send to the LLM\n",
    "    \"\"\"\n",
    "    # Combine the content of all documents\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "    \n",
    "    # Create the augmented prompt\n",
    "    prompt = f\"\"\"Use the following information to answer the user's question.\n",
    "If you can't find the answer in the provided information, say you don't know.\n",
    "Answer clearly and concisely.\n",
    "\n",
    "=== CONTEXT INFORMATION ===\n",
    "{context}\n",
    "=== END OF CONTEXT ===\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "print(\"‚úÖ create_augmented_prompt function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e36ad89",
   "metadata": {},
   "source": [
    "### üîç Let's See What an Augmented Prompt Looks Like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "39cf4c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù AUGMENTED PROMPT:\n",
      "============================================================\n",
      "Use the following information to answer the user's question.\n",
      "If you can't find the answer in the provided information, say you don't know.\n",
      "Answer clearly and concisely.\n",
      "\n",
      "=== CONTEXT INFORMATION ===\n",
      "4. LEAVE POLICY \n",
      "4.1 Annual Leave \n",
      "Full-time employees are entitled to paid annual leave based on their length of service. Leave accrual \n",
      "begins from the first day of employment. Employees must submit leave requests through the \n",
      "appropriate system and obtain approval from their supervisor before taking time off. \n",
      "4.2 Sick Leave \n",
      "Sick leave is provided to employees who are unable to work due to illness or injury. A medical certificate\n",
      "\n",
      "4. LEAVE POLICY \n",
      "4.1 Annual Leave \n",
      "Full-time employees are entitled to paid annual leave based on their length of service. Leave accrual \n",
      "begins from the first day of employment. Employees must submit leave requests through the \n",
      "appropriate system and obtain approval from their supervisor before taking time off. \n",
      "4.2 Sick Leave \n",
      "Sick leave is provided to employees who are unable to work due to illness or injury. A medical certificate\n",
      "=== END OF CONTEXT ===\n",
      "\n",
      "Question: What is the vacation policy?\n",
      "\n",
      "Answer:\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Create an example prompt\n",
    "example_question = \"What is the vacation policy?\"\n",
    "example_docs = vector_database.similarity_search(example_question, k=2)\n",
    "\n",
    "augmented_prompt = create_augmented_prompt(example_question, example_docs)\n",
    "\n",
    "print(\"üìù AUGMENTED PROMPT:\")\n",
    "print(\"=\" * 60)\n",
    "print(augmented_prompt)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d799b2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ü§ñ Step 9: Get the LLM Response\n",
    "\n",
    "### What Does This Step Do?\n",
    "\n",
    "We send the augmented prompt to the language model and get its response. This is the **\"G\" in RAG** - the **Generation**.\n",
    "\n",
    "### ü§ñ How to Ask the Jupyter Agent:\n",
    "```\n",
    "/generate Create a function that sends a prompt to the LLM and returns the response.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1e8b1634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ get_response function defined\n"
     ]
    }
   ],
   "source": [
    "def get_response(client, prompt):\n",
    "    \"\"\"\n",
    "    Sends the prompt to the LLM and gets the response.\n",
    "    \n",
    "    Args:\n",
    "        client: OpenAI client configured for LM Studio\n",
    "        prompt: The augmented prompt with context and question\n",
    "    \n",
    "    Returns:\n",
    "        String with the model's response, or None if there's an error\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are a helpful assistant that answers questions based on the provided context.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"lm-studio\",\n",
    "            messages=messages,\n",
    "            temperature=0.3,  # Lower = more consistent responses\n",
    "            max_tokens=500    # Response length limit\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error getting response: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ get_response function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252f3863",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîó Step 10: Create the Complete RAG Pipeline\n",
    "\n",
    "### What Is a Pipeline?\n",
    "\n",
    "It's a function that joins all the previous steps into a continuous flow:\n",
    "\n",
    "```\n",
    "Question ‚Üí Search ‚Üí Create Prompt ‚Üí Get Response ‚Üí Final Answer\n",
    "```\n",
    "\n",
    "### ü§ñ How to Ask the Jupyter Agent:\n",
    "```\n",
    "/generate Create a function that takes a question, searches for relevant documents, creates the prompt, and gets the LLM response.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "20775a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ rag_pipeline function defined\n"
     ]
    }
   ],
   "source": [
    "def rag_pipeline(question, database, client, num_documents=3):\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline: Retrieval ‚Üí Augmentation ‚Üí Generation\n",
    "    \n",
    "    Args:\n",
    "        question: The user's question\n",
    "        database: ChromaDB vector database\n",
    "        client: OpenAI client for LM Studio\n",
    "        num_documents: Number of documents to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        String with the model's response\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Starting RAG pipeline...\")\n",
    "    print(f\"\\nüìù Question: {question}\")\n",
    "    \n",
    "    # Step 1: RETRIEVAL - Search for relevant documents\n",
    "    print(\"\\nüîç Step 1: Searching for relevant documents...\")\n",
    "    documents = database.similarity_search(question, k=num_documents)\n",
    "    print(f\"   ‚úÖ Found {len(documents)} documents\")\n",
    "    \n",
    "    # Step 2: AUGMENTATION - Create the augmented prompt\n",
    "    print(\"\\nüìã Step 2: Creating augmented prompt...\")\n",
    "    prompt = create_augmented_prompt(question, documents)\n",
    "    print(\"   ‚úÖ Prompt created\")\n",
    "    \n",
    "    # Step 3: GENERATION - Get response from LLM\n",
    "    print(\"\\nü§ñ Step 3: Generating response...\")\n",
    "    response = get_response(client, prompt)\n",
    "    \n",
    "    if response:\n",
    "        print(\"   ‚úÖ Response generated\")\n",
    "    else:\n",
    "        print(\"   ‚ùå Error generating response\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"‚úÖ rag_pipeline function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7840046b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Step 11: Let's Test Our RAG System!\n",
    "\n",
    "Now let's ask questions about our documents and see the RAG system's responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f31715ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Starting RAG pipeline...\n",
      "\n",
      "üìù Question: What is the difference between the vacation policy and sick day policy?\n",
      "\n",
      "üîç Step 1: Searching for relevant documents...\n",
      "   ‚úÖ Found 3 documents\n",
      "\n",
      "üìã Step 2: Creating augmented prompt...\n",
      "   ‚úÖ Prompt created\n",
      "\n",
      "ü§ñ Step 3: Generating response...\n",
      "   ‚úÖ Response generated\n",
      "\n",
      "============================================================\n",
      "üì£ FINAL RESPONSE:\n",
      "============================================================\n",
      "The difference between the vacation policy and sick leave policy is as follows:\n",
      "\n",
      "* Annual Leave (Vacation): Full-time employees are entitled to paid annual leave based on their length of service. Leave accrual begins from the first day of employment, and employees must submit leave requests through the appropriate system and obtain approval from their supervisor before taking time off.\n",
      "* Sick Leave: Sick leave is provided to employees who are unable to work due to illness or injury. A medical certificate may be required for absences exceeding three consecutive working days, and sick leave should not be used for purposes other than legitimate medical needs.\n",
      "In summary, annual leave is for taking time off for personal reasons, while sick leave is for taking time off due to illness or injury.\n"
     ]
    }
   ],
   "source": [
    "# Ask a question about the document\n",
    "my_question = \"What is the difference between the vacation policy and sick day policy?\"\n",
    "\n",
    "# Run the RAG pipeline\n",
    "response = rag_pipeline(\n",
    "    question=my_question,\n",
    "    database=vector_database,\n",
    "    client=llm_client\n",
    ")\n",
    "\n",
    "# Display the response\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üì£ FINAL RESPONSE:\")\n",
    "print(\"=\" * 60)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116187b1",
   "metadata": {},
   "source": [
    "### üîÑ Let's Ask More Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52853f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with different questions\n",
    "test_questions = [\n",
    "    \"How many vacation days do I get per year?\",\n",
    "    \"What should I do if I'm sick?\",\n",
    "    \"Can I work from home?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(\"\\n\" + \"#\" * 70)\n",
    "    response = rag_pipeline(question, vector_database, llm_client)\n",
    "    print(\"\\nüì£ RESPONSE:\")\n",
    "    print(response)\n",
    "    print(\"#\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e9bc9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Summary: What Did We Learn?\n",
    "\n",
    "### The 3 Components of RAG:\n",
    "\n",
    "1. **R - Retrieval**\n",
    "   - We loaded documents (PDF)\n",
    "   - We split them into chunks\n",
    "   - We created embeddings and stored them in ChromaDB\n",
    "   - We searched for the most relevant chunks for each question\n",
    "\n",
    "2. **A - Augmentation**\n",
    "   - We combined the found documents with the question\n",
    "   - We created a prompt with context for the LLM\n",
    "\n",
    "3. **G - Generation**\n",
    "   - We sent the prompt to the LLM\n",
    "   - The model generated a response based on the context\n",
    "\n",
    "### Technologies Used:\n",
    "| Technology | Use |\n",
    "|------------|-----|\n",
    "| LM Studio | Run the LLM locally |\n",
    "| nomic-embed-text | Convert text to embeddings |\n",
    "| ChromaDB | Store and search embeddings |\n",
    "| LangChain | Orchestrate the entire process |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
