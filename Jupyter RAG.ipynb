{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c5c2fa5-f082-4afd-8345-e43571c77085",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install the necessary libraries for building a simple RAG system with PDFs and local AI\n",
    "# - pypdf: For handling PDF files\n",
    "# - sentence-transformers: For generating embeddings using Hugging Face models\n",
    "# - chromadb: For creating and managing a vector store\n",
    "# - requests: For making HTTP requests (used for interacting with APIs like LM Studio)\n",
    "\n",
    "%pip install -q pypdf sentence-transformers chromadb requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a94c2fd5-a2c7-44b6-b7c8-2c111079a5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules for building a RAG system\n",
    "\n",
    "# PdfReader: From the pypdf library, used to read and extract text from PDF files\n",
    "from pypdf import PdfReader\n",
    "\n",
    "# SentenceTransformer: From the sentence-transformers library, used to generate embeddings for text\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# chromadb: A library for creating and managing a vector store for embeddings\n",
    "import chromadb\n",
    "\n",
    "# requests: A library for making HTTP requests, used to interact with APIs like LM Studio\n",
    "import requests\n",
    "\n",
    "# os: A standard library module for interacting with the operating system (e.g., setting environment variables)\n",
    "import os\n",
    "\n",
    "# glob: A standard library module for finding file paths matching a specified pattern\n",
    "import glob\n",
    "\n",
    "# Set the path to the directory containing the documents (PDFs)\n",
    "DOCS_PATH = '/home/user/RAG Course Enhaced/Docs/'\n",
    "\n",
    "# Set the URL for the LM Studio API endpoint\n",
    "LM_STUDIO_URL = 'http://127.0.0.1:1234/v1/chat/completions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a0a8e1c-2129-4316-8065-89084d1c72e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 PDF files.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import glob  # For finding files matching a pattern\n",
    "from pypdf import PdfReader  # For reading and extracting text from PDF files\n",
    "\n",
    "# Set the path to the directory containing the documents (PDFs)\n",
    "DOCS_PATH = '/home/user/RAG Course Enhaced/Docs/'\n",
    "\n",
    "# Step 1: Find all PDF files in the specified directory\n",
    "pdf_files = glob.glob(f\"{DOCS_PATH}/*.pdf\")  # Use glob to find all .pdf files in DOCS_PATH\n",
    "\n",
    "# Step 2: Initialize an empty list to store the text from each PDF\n",
    "documents = []\n",
    "\n",
    "# Step 3: Loop through each PDF file\n",
    "for pdf_file in pdf_files:\n",
    "    # Step 4: Read the PDF file using PdfReader\n",
    "    reader = PdfReader(pdf_file)\n",
    "    \n",
    "    # Step 5: Extract text from all pages in the PDF\n",
    "    pdf_text = \"\"\n",
    "    for page in reader.pages:\n",
    "        pdf_text += page.extract_text()  # Append the text from each page to pdf_text\n",
    "    \n",
    "    # Step 6: Add the extracted text to the documents list\n",
    "    documents.append(pdf_text)\n",
    "\n",
    "# Step 7: Print how many PDFs were loaded\n",
    "print(f\"Loaded {len(pdf_files)} PDF files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2abeba1e-2387-4b83-ab1e-38e1f581410f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 32 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Define a function to split text into chunks of 1000 characters with 200-character overlap\n",
    "def split_text_into_chunks(text, chunk_size=1000, overlap=200):\n",
    "    \"\"\"\n",
    "    Splits the input text into chunks of a specified size with a specified overlap.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to be split.\n",
    "        chunk_size (int): The size of each chunk (default is 1000 characters).\n",
    "        overlap (int): The number of overlapping characters between consecutive chunks (default is 200 characters).\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])  # Add the chunk to the list\n",
    "        start += chunk_size - overlap  # Move the start index forward with overlap\n",
    "    return chunks\n",
    "\n",
    "# Initialize an empty list to store all chunks\n",
    "chunks = []\n",
    "\n",
    "# Loop through each document and split it into chunks\n",
    "for document in documents:\n",
    "    document_chunks = split_text_into_chunks(document)  # Split the document into chunks\n",
    "    chunks.extend(document_chunks)  # Add the chunks to the main chunks list\n",
    "\n",
    "# Print how many chunks were created\n",
    "print(f\"Created {len(chunks)} chunks.\")\n",
    "\n",
    "# Why do we need smaller pieces?\n",
    "# Splitting text into smaller chunks is important for several reasons:\n",
    "# 1. Many language models have a token limit, and smaller chunks ensure that the input fits within this limit.\n",
    "# 2. Smaller chunks improve the efficiency of processing and retrieval in systems like RAG (Retrieval-Augmented Generation).\n",
    "# 3. Overlapping chunks help preserve context between consecutive chunks, which is crucial for understanding and generating coherent responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07783be0-6497-4559-9f83-e6f9fa3472af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.55it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database ready!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sentence_transformers import SentenceTransformer  # For creating embeddings\n",
    "from chromadb import PersistentClient  # For interacting with Chroma database\n",
    "\n",
    "# Step 1: Load the SentenceTransformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Step 2: Create embeddings for all chunks\n",
    "# Encode all chunks using the SentenceTransformer model\n",
    "embeddings = model.encode(chunks, show_progress_bar=True)\n",
    "\n",
    "# Step 3: Initialize a Chroma client with PersistentClient\n",
    "# Set the path for the persistent database\n",
    "CHROMA_DB_PATH = './chroma_db'\n",
    "client = PersistentClient(path=CHROMA_DB_PATH)\n",
    "\n",
    "# Step 4: Create or get a collection called 'docs'\n",
    "collection = client.get_or_create_collection(name='docs')\n",
    "\n",
    "# Step 5: Add all chunks with their embeddings and IDs to the collection\n",
    "# Generate unique IDs for each chunk\n",
    "ids = [f\"chunk_{i}\" for i in range(len(chunks))]\n",
    "\n",
    "# Add chunks, their embeddings, and IDs to the collection\n",
    "collection.add(\n",
    "    ids=ids,  # Unique IDs for each chunk\n",
    "    documents=chunks,  # The text chunks\n",
    "    embeddings=embeddings  # The embeddings for the chunks\n",
    ")\n",
    "\n",
    "# Step 6: Print confirmation message\n",
    "print(\"Database ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a16a148f-7e16-4af0-beef-fa95893b662e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to ask a question\n",
    "def ask_question(question):\n",
    "    \n",
    "    # Step 1: Embed the question\n",
    "    # Use the SentenceTransformer model to create an embedding for the question\n",
    "    question_embedding = model.encode([question])[0]\n",
    "\n",
    "    # Step 2: Query the Chroma collection for the 3 most similar chunks\n",
    "    # Use the collection's query method to find the top 3 most similar chunks\n",
    "    results = collection.query(\n",
    "        query_embeddings=[question_embedding],\n",
    "        n_results=3  # Retrieve the top 3 most similar chunks\n",
    "    )\n",
    "\n",
    "    # Step 3: Combine the retrieved chunks into context text\n",
    "    # Extract the documents (chunks) from the query results\n",
    "    context_chunks = results['documents'][0]\n",
    "    context = \" \".join(context_chunks)  # Combine the chunks into a single context string\n",
    "\n",
    "    # Step 4: Create a prompt with the context and question\n",
    "    # The prompt instructs the model to answer using only the provided context\n",
    "    prompt = f\"Answer using only this context:\\n\\n{context}\\n\\nQuestion: {question}\"\n",
    "\n",
    "    # Step 5: Send the prompt to the language model\n",
    "    # Define the URL for the language model API\n",
    "    LM_STUDIO_URL = \"http://127.0.0.1:1234/v1/completions\"  # Updated URL\n",
    "\n",
    "    # Define the payload for the API request\n",
    "    payload = {\n",
    "        \"model\": \"llama-2-7b-chat:2\",  # Specify the model to use\n",
    "        \"prompt\": prompt,  # Provide the prompt\n",
    "        \"temperature\": 0.1  # Set the temperature for response generation\n",
    "    }\n",
    "\n",
    "    # Send the request to the language model API\n",
    "    response = requests.post(LM_STUDIO_URL, json=payload)\n",
    "\n",
    "    # Step 6: Print the AI answer\n",
    "    # Extract and print the AI's response from the API response\n",
    "    if response.status_code == 200:\n",
    "        ai_answer = response.json().get(\"choices\", [{}])[0].get(\"text\", \"\").strip()\n",
    "        print(\"AI Answer:\", ai_answer)\n",
    "    else:\n",
    "        print(\"Error:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49f0f079-132f-44bc-91b8-41668d15c27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the main topic of these documents?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Answer: Answer: The main topic of these documents is data classification and handling policies for a company.\n",
      "--------------------------------------------------\n",
      "Question: Summarize the key points from the content\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Answer: provided in the context of an employee handbook.\n",
      "Answer: The key points from the content provided in the context of an employee handbook are:\n",
      "1. Bereavement Leave: Employees may be granted paid bereavement leave in the event of the death of an immediate family member.\n",
      "2. Workplace Health and Safety: The organization is committed to providing a safe and healthy work environment, and employees are expected to actively participate in maintaining safe working conditions.\n",
      "3. Internet Usage: Internet access is provided for business purposes, but streaming media services, social media, and other bandwidth-intensive applications should be used sparingly during business hours. Accessing or downloading inappropriate, illegal, or malicious content is strictly prohibited.\n",
      "4. Data Classification and Handling: All information assets must be classified according to their sensitivity and criticality, and employees must return all company property upon termination of employment.\n",
      "5. Return of Company Property: Upon termination of employment, employees must return all company property including identification badges, keys, equipment, documents, and any other materials in their possession. Failure to return company property may result in deductions from final pay as permitted by law.\n",
      "6. Final Pay: Final pay will be processed in accordance with applicable state and federal laws, including payment for all hours worked, accrued but unused vacation time as applicable, and any other earned compensation. Information about benefit continuation options will be provided.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Testing our minimal RAG (Retrieval-Augmented Generation) system\n",
    "\n",
    "# Define the test questions\n",
    "test_questions = [\n",
    "    \"What is the main topic of these documents?\",\n",
    "    \"Summarize the key points from the content\"\n",
    "]\n",
    "\n",
    "# Loop through each question and test the ask_question function\n",
    "for question in test_questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    ask_question(question)  # Call the ask_question function\n",
    "    print(\"-\" * 50)  # Separator for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bcede3-bdaa-4999-b252-7be689d89826",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
